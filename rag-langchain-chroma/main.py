# main.py
import os
from pathlib import Path

from dotenv import load_dotenv
from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma

# LCEL ç‰ˆï¼šå…¬å¼ãŒæ¨å¥¨ã™ã‚‹æœ€æ–°ã®çµ„ã¿æ–¹
from langchain.chains.combine_documents.stuff import create_stuff_documents_chain
from langchain.prompts import ChatPromptTemplate
from langchain.chains.retrieval import create_retrieval_chain  # æ­£ã—ã„ import ãƒ‘ã‚¹ã«æ³¨æ„ :contentReference[oaicite:2]{index=2}

load_dotenv()

DOCS_DIR = "docs"
VECTOR_DIR = "vector_store"

def build_or_load_vectordb():
    """VectorDB ãŒç„¡ã‘ã‚Œã°ä½œã£ã¦è¿”ã™ã€‚ã‚ã‚Œã°ãƒ­ãƒ¼ãƒ‰ã—ã¦è¿”ã™ã€‚"""
    embeddings = OpenAIEmbeddings()  # HuggingFaceEmbeddings ã«å·®ã—æ›¿ãˆå¯
    if Path(VECTOR_DIR).exists():
        return Chroma(persist_directory=VECTOR_DIR,
                      embedding_function=embeddings)
    # --- ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ–°è¦ä½œæˆ ---
    loader = DirectoryLoader(
        DOCS_DIR,
        glob="**/*.md",
        loader_cls=TextLoader,      # md â†’ æ–‡å­—åˆ—
        loader_kwargs={"encoding": "utf-8"},
        show_progress=True,
        use_multithreading=True
    )
    docs = loader.load()
    splitter = RecursiveCharacterTextSplitter(chunk_size=1000,
                                              chunk_overlap=200)
    split_docs = splitter.split_documents(docs)
    vectordb = Chroma.from_documents(split_docs,
                                     embeddings,
                                     persist_directory=VECTOR_DIR)
    return vectordb

def build_rag_chain(vectordb):
    retriever = vectordb.as_retriever(search_kwargs={"k": 4})

    # ---- ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆçµåˆç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ ----
    system_prompt = (
        "ã‚ãªãŸã¯ Markdown è³‡æ–™ã®å°‚é–€å®¶ã§ã™ã€‚"
        "ä»¥ä¸‹ã® <context></context> å†…ã ã‘ã‚’æ ¹æ‹ ã«æ—¥æœ¬èªã§ç°¡æ½”ã«ç­”ãˆã¦ãã ã•ã„ã€‚"
    )
    prompt = ChatPromptTemplate.from_messages(
        [("system", system_prompt),
         ("human", "<context>{context}</context>\n\nè³ªå•: {input}")]
    )

    combine_chain = create_stuff_documents_chain(
        llm=ChatOpenAI(model_name="gpt-4o-mini", temperature=0),
        prompt=prompt,
    )
    return create_retrieval_chain(retriever, combine_chain)  # RetrievalQA ã¯ 0.1.17 ã§éæ¨å¥¨ :contentReference[oaicite:3]{index=3}

def interactive_cli(chain):
    print("=== RAG QA CLI ===   'exit' ã§çµ‚äº†")
    while True:
        q = input("â“> ")
        if q.lower() in {"exit", "quit"}:
            break
        ans = chain.invoke({"input": q})
        print("ğŸ’¡", ans["answer"])
        # å‡ºå…¸æ–‡æ›¸ã‚’è¦‹ãŸã„å ´åˆ:
        # for doc in ans["context"]:
        #     print("ğŸ“„", doc.metadata["source"])

if __name__ == "__main__":
    vectordb = build_or_load_vectordb()
    rag_chain = build_rag_chain(vectordb)
    interactive_cli(rag_chain)
